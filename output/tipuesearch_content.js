var tipuesearch = {"pages":[{"title":"  Search Code Snippets\n","text":"\n\n\n  Search Code Snippets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Snippets\n\n\nHome\nSearch\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n","tags":"","url":"search/index.html"},{"title":"Creating AWS MQ (RabbitMQ) with Terraform","text":"AWS Secrets Manager is used to store the RabbitMQ admin password. terraform { required_version = \">= 1.2.0\" required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 4.16\" } } provider \"aws\" { region = var.aws-region } data \"aws_vpc\" \"vpc\" { id = var.vpc-id } data \"aws_subnets\" \"private\" { filter { name = \"vpc-id\" values = [ data.aws_vpc.vpc.id ] } filter { # Assumes private=1 is the tag for private subnets # Adjust based on exact subnet setup name = \"private\" values = [ \"1\" ] } } data \"aws_secretsmanager_secret\" \"rabbitmq_secret\" { name = \"rabbitmq-password\" } data \"aws_secretsmanager_secret_version\" \"rabbitmq_secret_version\" { secret_id = data.aws_secretsmanager_secret.rabbitmq_secret.id } resource \"aws_mq_broker\" \"rabbit\" { broker_name = var.mq-broker-name engine_type = \"RabbitMQ\" # Change to latest if out of date engine_version = \"3.10.20\" host_instance_type = \"mq.t3.micro\" publicly_accessible = false security_groups = [ ... ] # Fixed to one subnet as SINGLE_INSTANCE is used # Expand to use more or all subnets in a multi node setup subnet_ids = [ data.aws_subnets.private.ids[0 ]] deployment_mode = \"SINGLE_INSTANCE\" user { username = \"admin\" password = jsondecode ( data.aws_secretsmanager_secret_version.rabbitmq_secret_version.secret_string )[ \"admin\" ] } } # Host and IP are not direct properties of aws_mq_broker # for RabbitMQ, so examples below show one way to extract module \"shell_ip\" { source = \"Invicton-Labs/shell-resource/external\" command_unix = \"dig +short $(echo $URL | cut -d'/' -f3 | cut -d':' -f1) | grep -v '\\\\.$'\" environment = { URL = aws_mq_broker.mq.instances.0.console_url } depends_on = [ aws_mq_broker.mq ] } module \"shell_host\" { source = \"Invicton-Labs/shell-resource/external\" command_unix = \"echo $URL | cut -d'/' -f3 | cut -d':' -f1\" environment = { URL = aws_mq_broker.mq.instances.0.console_url } depends_on = [ aws_mq_broker.mq ] } output \"mq-host\" { value = module.shell_host.stdout } output \"mq-ip\" { value = module.shell_ip.stdout } output \"mq-console\" { value = aws_mq_broker.mq.instances.0.console_url }","tags":"Cloud","url":"/creating-aws-mq-rabbitmq-with-terraform/","loc":"/creating-aws-mq-rabbitmq-with-terraform/"},{"title":"Deploying to Kubernetes ( EKS ) via Terraform","text":"Prerequisites: AWS cli configure with AWS keys kubectl command installed Kubernetes set up on AWS via EKS (change host and cluster_ca_certificate if Kubernetes set up another way) data \"aws_eks_cluster\" \"eks\" { name = var.eks-cluster-name } provider \"kubectl\" { load_config_file = false host = data.aws_eks_cluster.eks.endpoint cluster_ca_certificate = base64decode ( data.aws_eks_cluster.eks.certificate_authority.0.data ) exec { api_version = \"client.authentication.k8s.io/v1beta1\" args = [ \"eks\", \"get-token\", \"--cluster-name\" , data.aws_eks_cluster.cluster.id ] command = \"aws\" } } data \"kubectl_path_documents\" \"manifests\" { pattern = \"path/to/yaml/*.yml\" } resource \"kubectl_manifest\" \"manifest\" { count = length ( data.kubectl_path_documents.manifests.documents ) yaml_body = element ( data.kubectl_path_documents.manifests.documents , count.index ) wait_for_rollout = true depends_on = [ kubernetes_manifest.config ] } /* Example reference to AWS resource to pass to service via config block below. */ data \"aws_s3_bucket\" \"bucket\" { bucket = \"my-bucket\" } /* Optional block to pass env vars to service. Delete and the `depends_on` above if not needed. Example belows show passing hostname of S3 bucket. */ resource \"kubernetes_manifest\" \"config\" { manifest = { \"apiVersion\" = \"v1\" \"kind\" = \"ConfigMap\" \"metadata\" = { \"name\" = \"config\" \"namespace\" = \"dsdservice\" } \"data\" = { \"S3_BUCKET_HOST\" = data.aws_s3_bucket.bucket.bucket_domain_name } } }","tags":"Cloud","url":"/deploying-to-kubernetes-eks-via-terraform/","loc":"/deploying-to-kubernetes-eks-via-terraform/"},{"title":"Kafka with AWS MSK and an S3 archive via Kafka Connect","text":"terraform { required_providers { kafka = { source = \"Mongey/kafka\" version = \"0.5.3\" } http = { source = \"hashicorp/http\" } } } variable \"vpc-id\" { type = string } variable \"msk-cluster-name\" { type = string } variable \"archive-bucket-name\" { type = string } variable \"archived-topics\" { type = list ( string ) } data \"aws_vpc\" \"vpc\" { id = var.vpc-id } data \"aws_subnets\" \"private\" { filter { name = \"vpc-id\" values = [ data.aws_vpc.vpc.id ] } filter { # Assumes private=1 is the tag for private subnets # Adjust based on exact subnet setup name = \"private\" values = [ \"1\" ] } } # Allows access from all traffic in VPC private subnets # Adjust if more fine-grained security is required module \"security_group\" { source = \"terraform-aws-modules/security-group/aws\" version = \"~> 5.0\" name = \"${var.msk-cluster-name}-access\" description = \"Security group for MSK ${var.msk-cluster-name}\" vpc_id = data.aws_vpc.vpc.id ingress_cidr_blocks = [ data.aws_vpc.vpc.cidr_block ] ingress_rules = [ \"kafka-broker-tcp\" , \"kafka-broker-tls-tcp\" ] } resource \"aws_s3_bucket\" \"archive\" { bucket = var.archive-bucket-name } resource \"aws_s3_bucket_lifecycle_configuration\" \"expiration_policy\" { bucket = aws_s3_bucket.archive.id rule { status = \"Enabled\" expiration { # Adjust as desired days = 90 } filter { prefix = \"topics/\" } id = \"topics\" } } data \"http\" \"file\" { # Note version needs updating when appropriate url = \"https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.5.1/confluentinc-kafka-connect-s3-10.5.1.zip\" } # Pretend contents are sensitive to avoid it printing binary data resource \"local_sensitive_file\" \"confluentinc-kafka-connect-s3\" { content_base64 = data.http.file.response_body_base64 filename = \"confluentinc-kafka-connect-s3.zip\" } resource \"aws_s3_object\" \"s3_sink_msk_connect_custom_plugin_code\" { bucket = aws_s3_bucket.archive.id key = local_sensitive_file.confluentinc-kafka-connect-s3.filename source = local_sensitive_file.confluentinc-kafka-connect-s3.filename etag = local_sensitive_file.confluentinc-kafka-connect-s3.content_sha512 } module \"msk_cluster\" { source = \"terraform-aws-modules/msk-kafka-cluster/aws\" name = var.msk-cluster-name kafka_version = \"3.2.0\" # Adjust as desired number_of_broker_nodes = 4 # Adjust as desired so long as it matches no. of nodes broker_node_client_subnets = [ data.aws_subnets.subnets.ids.0 , data.aws_subnets.subnets.ids.1 ] # Adjust as desired broker_node_instance_type = \"kafka.m5.large\" # Adjust as desired broker_node_storage_info = { ebs_storage_info = { volume_size = 100 } } broker_node_security_groups = [ module.security_group.security_group_id ] configuration_name = \"${var.msk-cluster-name}-config\" # Adjust as desired configuration_server_properties = { \"auto.create.topics.enable\" = true \"default.replication.factor\" = 2 \"min.insync.replicas\" = 1 \"num.io.threads\" = 12 \"num.network.threads\" = 10 \"num.partitions\" = 24 \"num.replica.fetchers\" = 2 \"replica.lag.time.max.ms\" = 30000 \"unclean.leader.election.enable\" = true \"zookeeper.session.timeout.ms\" = 18000 \"log.retention.hours\" = 3 } connect_custom_plugins = { s3_sink = { name = \"${var.msk-cluster-name}-s3-sink\" description = \"Custom Plugin for S3 Sink\" content_type = \"ZIP\" s3_bucket_arn = aws_s3_bucket.archive.arn s3_file_key = aws_s3_object.s3_sink_msk_connect_custom_plugin_code.key timeouts = { create = \"60m\" } } } } resource \"aws_mskconnect_connector\" \"s3_sink\" { name = \"${var.msk-cluster-name}-archiver\" kafkaconnect_version = \"2.7.1\" capacity { provisioned_capacity { worker_count = 1 } } # Assumes JSON data and partitioned by the hour # See https://docs.confluent.io/kafka-connectors/s3-sink/current/configuration_options.html connector_configuration = { \"connector.class\" = \"io.confluent.connect.s3.S3SinkConnector\" \"s3.region\" = \"us-west-2\" , \"format.class\" = \"io.confluent.connect.s3.format.bytearray.ByteArrayFormat\" , \"value.converter\" = \"org.apache.kafka.connect.converters.ByteArrayConverter\" , \"format.bytearray.extension\" = \".json\" , \"path.format\" = \"'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH\" \"flush.size\" = \"1\" , \"schema.compatibility\" = \"NONE\" , \"topics\" = var.archived-topics , \"tasks.max\" = \"1\" , \"partitioner.class\" = \"io.confluent.connect.storage.partitioner.TimeBasedPartitioner\" , \"locale\" = \"en-GB\" , \"timezone\" = \"UTC\" , \"partition.duration.ms\" = \"3600000\" \"storage.class\" = \"io.confluent.connect.s3.storage.S3Storage\" , \"s3.bucket.name\" = aws_s3_bucket.archive.id , } kafka_cluster { apache_kafka_cluster { bootstrap_servers = module.msk_cluster.bootstrap_brokers_tls vpc { security_groups = [ module.security_group.security_group_id ] subnets = [ data.aws_subnets.subnets.ids.0 , data.aws_subnets.subnets.ids.1 ] } } } kafka_cluster_client_authentication { authentication_type = \"NONE\" } kafka_cluster_encryption_in_transit { encryption_type = \"TLS\" } plugin { custom_plugin { arn = module.msk_cluster.connect_custom_plugins.s3_sink.arn revision = module.msk_cluster.connect_custom_plugins.s3_sink.latest_revision } } log_delivery { worker_log_delivery { cloudwatch_logs { enabled = true log_group = aws_cloudwatch_log_group.kafka-archiver-logs.name } } } service_execution_role_arn = aws_iam_role.connector_role.arn } resource \"aws_cloudwatch_log_group\" \"kafka-archiver-logs\" { name = \"${var.msk-cluster-name}-archiver\" } resource \"aws_iam_role\" \"connector_role\" { name = \"${var.msk-cluster-name}-archiver-role\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"kafkaconnect.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] }) } resource \"aws_iam_role_policy\" \"connector_role_policy\" { name = \"${var.msk-cluster-name}-archiver-role-policy\" role = aws_iam_role.connector_role.id policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListAllMyBuckets\" ], \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" , \"s3:GetBucketLocation\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:ListMultipartUploadParts\" , \"s3:ListBucketMultipartUploads\" ], \"Resource\" : \"*\" } ] }) }","tags":"Cloud","url":"/kafka-with-aws-msk-and-an-s3-archive-via-kafka-connect/","loc":"/kafka-with-aws-msk-and-an-s3-archive-via-kafka-connect/"},{"title":"Python Flask Dockerfile","text":"# syntax=docker/dockerfile:1 FROM python:3.8-slim-buster WORKDIR /python-docker # Copy just requirements first to create cached layer with deps COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt # Now copy the rest of the source COPY . . CMD [ \"python3\" , \"-m\" , \"flask\" , \"run\" , \"--host=0.0.0.0\" ] Notes: Creating a .dockerignore is recommended to prevent the COPY . . line from copying too much (e.g. if you have a venv in the working directory)","tags":"Python","url":"/python-flask-dockerfile/","loc":"/python-flask-dockerfile/"},{"title":"Quarkus Multistage Dockerfile with Maven","text":"--- tags : quarkus, java, maven, docker, dockerfile --- FROM quay.io/quarkus/centos-quarkus-maven:19.2.1 AS build COPY ./pom.xml ./pom.xml COPY ./src ./src RUN mvn -Pnative package -DskipTests FROM registry.access.redhat.com/ubi8/ubi-minimal WORKDIR /work/ COPY --from = build /project/target/*-runner /work/application RUN chmod 775 /work EXPOSE 8080 CMD [ \"./application\" , \"-Dquarkus.http.host=0.0.0.0\" ]","tags":"Java","url":"/quarkus-multistage-dockerfile-with-maven/","loc":"/quarkus-multistage-dockerfile-with-maven/"},{"title":"Quarkus Native Multistage Dockerfile with Maven","text":"## Stage 1 : build with maven builder image with native capabilities FROM quay.io/quarkus/ubi-quarkus-mandrel-builder-image:jdk-17 AS build COPY --chown = quarkus:quarkus mvnw /code/mvnw COPY --chown = quarkus:quarkus .mvn /code/.mvn COPY --chown = quarkus:quarkus pom.xml /code/ USER quarkus WORKDIR /code RUN ./mvnw -B org.apache.maven.plugins:maven-dependency-plugin:3.1.2:go-offline COPY src /code/src RUN ./mvnw package -Dnative ## Stage 2 : create the docker final image FROM quay.io/quarkus/quarkus-micro-image:2.0 WORKDIR /work/ COPY --from = build /code/target/*-runner /work/application # set up permissions for user `1001` RUN chmod 775 /work /work/application \\ && chown -R 1001 /work \\ && chmod -R \"g+rwX\" /work \\ && chown -R 1001 :root /work EXPOSE 8080 USER 1001 CMD [ \"./application\" , \"-Dquarkus.http.host=0.0.0.0\" ]","tags":"Java","url":"/quarkus-native-multistage-dockerfile-with-maven/","loc":"/quarkus-native-multistage-dockerfile-with-maven/"},{"title":"Using direnv on a Python project","text":"Prerequisites direnv is installed Snippet Create .envrc in project root: # Using direnv built-in to create venv layout python3 # Create venv as alias to latest # Useful for configuring ./venv as a fix dir in Pycharm etc. ln -sfn .direnv/ $( basename $VIRTUAL_ENV ) / venv # Optionally look for a 2nd file containing passwords # e.g. export API_KEY=xxx if file .envrc.secret ; then source_env .envrc.secret fi","tags":"Python","url":"/using-direnv-on-a-python-project/","loc":"/using-direnv-on-a-python-project/"},{"title":"CodeSnipsPro Home","text":"Working code snippets for the busy professional. Creating AWS MQ (RabbitMQ) with Terraform Deploying to Kubernetes ( EKS ) via Terraform Kafka with AWS MSK and an S3 archive via Kafka Connect Quarkus Multistage Dockerfile with Maven Quarkus Native Multistage Dockerfile with Maven Python Flask Dockerfile Using direnv on a Python project","tags":"misc","url":"/_index/","loc":"/_index/"}]};