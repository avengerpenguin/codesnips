var tipuesearch = {"pages":[{"title":"  Search CodeSnips\n","text":"\n\n\n  Search CodeSnips\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeSnips\n\n\nHome\nSearch\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n","tags":"","url":"https://codesnips.pro/search/index.html"},{"title":"Add Homebrew bin path on Mac to emacs $ PATH","text":"Homebrew likes to install things in /usr/local/bin so if you want to call commands from Emacs add the following to $HOME/.emacs.d/init.el : ( setenv \"PATH\" ( concat \"/usr/local/bin:\" ( getenv \"PATH\" )))","tags":"Configs","url":"https://codesnips.pro/add-homebrew-bin-path-on-mac-to-emacs-path/","loc":"https://codesnips.pro/add-homebrew-bin-path-on-mac-to-emacs-path/"},{"title":"Add key binding to delete currently-playing file in mpv","text":"Config to bind Ctrl+ DEL to mark a video for deletion while watching it. Press again to unmark it. Deletion is performed when mpv exits. This is useful for sorting through e.g. videos and photos taken on a phone where you want to clear out things you don't want to keep but do so while playing/viewing them. Save as e.g. $HOME/.config/mpv/scripts/delete_file.lua local utils = require \"mp.utils\" del_list = {} function contains_item ( l , i ) for k , v in pairs ( l ) do if v == i then mp . osd_message ( \"undeleting current file\" ) l [ k ] = nil return true end end mp . osd_message ( \"deleting current file\" ) return false end function mark_delete () local work_dir = mp . get_property_native ( \"working-directory\" ) local file_path = mp . get_property_native ( \"path\" ) local s = file_path : find ( work_dir , 0 , true ) local final_path if s and s == 0 then final_path = file_path else final_path = utils . join_path ( work_dir , file_path ) end if not contains_item ( del_list , final_path ) then table.insert ( del_list , final_path ) end end function delete () for i , v in pairs ( del_list ) do print ( \"deleting: \" .. v ) os.remove ( v ) end end mp . add_key_binding ( \"ctrl+DEL\" , \"delete_file\" , mark_delete ) mp . register_event ( \"shutdown\" , delete )","tags":"Configs","url":"https://codesnips.pro/add-key-binding-to-delete-currently-playing-file-in-mpv/","loc":"https://codesnips.pro/add-key-binding-to-delete-currently-playing-file-in-mpv/"},{"title":"Adding volume control keybindings to Xmonad","text":"Sets mod-F6 and mod-F7 to volume down/up respectively. Example below sets \"Windows\" meta key as mod key, but configure as preferred. myModMask = mod4Mask main = xmonad $ def { modMask = myModMask } ` additionalKeys ` [ (( myModMask , xK_F6 ), spawn \"amixer -q sset Master 5%-\" ) , (( myModMask , xK_F7 ), spawn \"amixer -q sset Master 5%+\" ) ]","tags":"Configs","url":"https://codesnips.pro/adding-volume-control-keybindings-to-xmonad/","loc":"https://codesnips.pro/adding-volume-control-keybindings-to-xmonad/"},{"title":"Cloud","text":"AWS Creating AWS MQ (RabbitMQ) with Terraform Deploying to Kubernetes ( EKS ) via Terraform Kafka with AWS MSK and an S3 archive via Kafka Connect Finding your AWS account ID via aws cli","tags":"Cloud","url":"https://codesnips.pro/cloud/","loc":"https://codesnips.pro/cloud/"},{"title":"Configs","text":"emacs Add Homebrew bin path on Mac to emacs $ PATH mpv Add key binding to delete currently-playing file in mpv xmonad Adding volume control keybindings to Xmonad xterm Setting the font in xterm","tags":"Configs","url":"https://codesnips.pro/configs/","loc":"https://codesnips.pro/configs/"},{"title":"Creating AWS MQ (RabbitMQ) with Terraform","text":"AWS Secrets Manager is used to store the RabbitMQ admin password. terraform { required_version = \">= 1.2.0\" required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 4.16\" } } provider \"aws\" { region = var.aws-region } data \"aws_vpc\" \"vpc\" { id = var.vpc-id } data \"aws_subnets\" \"private\" { filter { name = \"vpc-id\" values = [ data.aws_vpc.vpc.id ] } filter { # Assumes private=1 is the tag for private subnets # Adjust based on exact subnet setup name = \"private\" values = [ \"1\" ] } } data \"aws_secretsmanager_secret\" \"rabbitmq_secret\" { name = \"rabbitmq-password\" } data \"aws_secretsmanager_secret_version\" \"rabbitmq_secret_version\" { secret_id = data.aws_secretsmanager_secret.rabbitmq_secret.id } resource \"aws_mq_broker\" \"rabbit\" { broker_name = var.mq-broker-name engine_type = \"RabbitMQ\" # Change to latest if out of date engine_version = \"3.10.20\" host_instance_type = \"mq.t3.micro\" publicly_accessible = false security_groups = [ ... ] # Fixed to one subnet as SINGLE_INSTANCE is used # Expand to use more or all subnets in a multi node setup subnet_ids = [ data.aws_subnets.private.ids[0 ]] deployment_mode = \"SINGLE_INSTANCE\" user { username = \"admin\" password = jsondecode ( data.aws_secretsmanager_secret_version.rabbitmq_secret_version.secret_string )[ \"admin\" ] } } # Host and IP are not direct properties of aws_mq_broker # for RabbitMQ, so examples below show one way to extract module \"shell_ip\" { source = \"Invicton-Labs/shell-resource/external\" command_unix = \"dig +short $(echo $URL | cut -d'/' -f3 | cut -d':' -f1) | grep -v '\\\\.$'\" environment = { URL = aws_mq_broker.mq.instances.0.console_url } depends_on = [ aws_mq_broker.mq ] } module \"shell_host\" { source = \"Invicton-Labs/shell-resource/external\" command_unix = \"echo $URL | cut -d'/' -f3 | cut -d':' -f1\" environment = { URL = aws_mq_broker.mq.instances.0.console_url } depends_on = [ aws_mq_broker.mq ] } output \"mq-host\" { value = module.shell_host.stdout } output \"mq-ip\" { value = module.shell_ip.stdout } output \"mq-console\" { value = aws_mq_broker.mq.instances.0.console_url }","tags":"Cloud","url":"https://codesnips.pro/creating-aws-mq-rabbitmq-with-terraform/","loc":"https://codesnips.pro/creating-aws-mq-rabbitmq-with-terraform/"},{"title":"Deploying to Kubernetes ( EKS ) via Terraform","text":"Prerequisites: AWS cli configure with AWS keys kubectl command installed Kubernetes set up on AWS via EKS (change host and cluster_ca_certificate if Kubernetes set up another way) data \"aws_eks_cluster\" \"eks\" { name = var.eks-cluster-name } provider \"kubectl\" { load_config_file = false host = data.aws_eks_cluster.eks.endpoint cluster_ca_certificate = base64decode ( data.aws_eks_cluster.eks.certificate_authority.0.data ) exec { api_version = \"client.authentication.k8s.io/v1beta1\" args = [ \"eks\", \"get-token\", \"--cluster-name\" , data.aws_eks_cluster.cluster.id ] command = \"aws\" } } data \"kubectl_path_documents\" \"manifests\" { pattern = \"path/to/yaml/*.yml\" } resource \"kubectl_manifest\" \"manifest\" { count = length ( data.kubectl_path_documents.manifests.documents ) yaml_body = element ( data.kubectl_path_documents.manifests.documents , count.index ) wait_for_rollout = true depends_on = [ kubernetes_manifest.config ] } /* Example reference to AWS resource to pass to service via config block below. */ data \"aws_s3_bucket\" \"bucket\" { bucket = \"my-bucket\" } /* Optional block to pass env vars to service. Delete and the `depends_on` above if not needed. Example belows show passing hostname of S3 bucket. */ resource \"kubernetes_manifest\" \"config\" { manifest = { \"apiVersion\" = \"v1\" \"kind\" = \"ConfigMap\" \"metadata\" = { \"name\" = \"config\" \"namespace\" = \"dsdservice\" } \"data\" = { \"S3_BUCKET_HOST\" = data.aws_s3_bucket.bucket.bucket_domain_name } } }","tags":"Cloud","url":"https://codesnips.pro/deploying-to-kubernetes-eks-via-terraform/","loc":"https://codesnips.pro/deploying-to-kubernetes-eks-via-terraform/"},{"title":"Finding your AWS account ID via aws cli","text":"Sometimes you need to use your account ID in scripts so the following allows you to find it automatically. aws sts get-caller-identity --query \"Account\" --output text You can use it in a script like this: AWS_ACCOUNT_ID = $( aws sts get-caller-identity --query \"Account\" --output text ) See it in use in Pushing a docker image to AWS Elastic Container Registry ( ECR ) .","tags":"Cloud","url":"https://codesnips.pro/finding-your-aws-account-id-via-aws-cli/","loc":"https://codesnips.pro/finding-your-aws-account-id-via-aws-cli/"},{"title":"Hello World Flask App","text":"Create hello.py : from flask import Flask , render_template , request # Create a Flask app instance app = Flask ( __name__ ) # Route for the home page @app . route ( '/' ) def home (): return 'Hello, World!' # Route for a simple form @app . route ( '/form' , methods = [ 'GET' , 'POST' ]) def form (): if request . method == 'POST' : name = request . form . get ( 'name' ) return f 'Hello, { name } !' return render_template ( 'form.html' ) # Route with dynamic content @app . route ( '/greet/<name>' ) def greet ( name ): return f 'Hello, { name } !' # Run the app if this script is executed directly if __name__ == '__main__' : app . run ( debug = True ) Create templates/form.html : <!DOCTYPE html> < html > < head > < title > Simple Form </ title > </ head > < body > < h1 > Enter your name: </ h1 > < form method = \"POST\" action = \"/form\" > < input type = \"text\" name = \"name\" required > < input type = \"submit\" value = \"Submit\" > </ form > </ body > </ html > Run: flask --app = hello run","tags":"Python","url":"https://codesnips.pro/hello-world-flask-app/","loc":"https://codesnips.pro/hello-world-flask-app/"},{"title":"Java","text":"Quarkus Quarkus Hello World Web app Quarkus Multistage Dockerfile with Maven Quarkus Native Multistage Dockerfile with Maven","tags":"Java","url":"https://codesnips.pro/java/","loc":"https://codesnips.pro/java/"},{"title":"Kafka with AWS MSK and an S3 archive via Kafka Connect","text":"terraform { required_providers { kafka = { source = \"Mongey/kafka\" version = \"0.5.3\" } http = { source = \"hashicorp/http\" } } } variable \"vpc-id\" { type = string } variable \"msk-cluster-name\" { type = string } variable \"archive-bucket-name\" { type = string } variable \"archived-topics\" { type = list ( string ) } data \"aws_vpc\" \"vpc\" { id = var.vpc-id } data \"aws_subnets\" \"private\" { filter { name = \"vpc-id\" values = [ data.aws_vpc.vpc.id ] } filter { # Assumes private=1 is the tag for private subnets # Adjust based on exact subnet setup name = \"private\" values = [ \"1\" ] } } # Allows access from all traffic in VPC private subnets # Adjust if more fine-grained security is required module \"security_group\" { source = \"terraform-aws-modules/security-group/aws\" version = \"~> 5.0\" name = \"${var.msk-cluster-name}-access\" description = \"Security group for MSK ${var.msk-cluster-name}\" vpc_id = data.aws_vpc.vpc.id ingress_cidr_blocks = [ data.aws_vpc.vpc.cidr_block ] ingress_rules = [ \"kafka-broker-tcp\" , \"kafka-broker-tls-tcp\" ] } resource \"aws_s3_bucket\" \"archive\" { bucket = var.archive-bucket-name } resource \"aws_s3_bucket_lifecycle_configuration\" \"expiration_policy\" { bucket = aws_s3_bucket.archive.id rule { status = \"Enabled\" expiration { # Adjust as desired days = 90 } filter { prefix = \"topics/\" } id = \"topics\" } } data \"http\" \"file\" { # Note version needs updating when appropriate url = \"https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.5.1/confluentinc-kafka-connect-s3-10.5.1.zip\" } # Pretend contents are sensitive to avoid it printing binary data resource \"local_sensitive_file\" \"confluentinc-kafka-connect-s3\" { content_base64 = data.http.file.response_body_base64 filename = \"confluentinc-kafka-connect-s3.zip\" } resource \"aws_s3_object\" \"s3_sink_msk_connect_custom_plugin_code\" { bucket = aws_s3_bucket.archive.id key = local_sensitive_file.confluentinc-kafka-connect-s3.filename source = local_sensitive_file.confluentinc-kafka-connect-s3.filename etag = local_sensitive_file.confluentinc-kafka-connect-s3.content_sha512 } module \"msk_cluster\" { source = \"terraform-aws-modules/msk-kafka-cluster/aws\" name = var.msk-cluster-name kafka_version = \"3.2.0\" # Adjust as desired number_of_broker_nodes = 4 # Adjust as desired so long as it matches no. of nodes broker_node_client_subnets = [ data.aws_subnets.subnets.ids.0 , data.aws_subnets.subnets.ids.1 ] # Adjust as desired broker_node_instance_type = \"kafka.m5.large\" # Adjust as desired broker_node_storage_info = { ebs_storage_info = { volume_size = 100 } } broker_node_security_groups = [ module.security_group.security_group_id ] configuration_name = \"${var.msk-cluster-name}-config\" # Adjust as desired configuration_server_properties = { \"auto.create.topics.enable\" = true \"default.replication.factor\" = 2 \"min.insync.replicas\" = 1 \"num.io.threads\" = 12 \"num.network.threads\" = 10 \"num.partitions\" = 24 \"num.replica.fetchers\" = 2 \"replica.lag.time.max.ms\" = 30000 \"unclean.leader.election.enable\" = true \"zookeeper.session.timeout.ms\" = 18000 \"log.retention.hours\" = 3 } connect_custom_plugins = { s3_sink = { name = \"${var.msk-cluster-name}-s3-sink\" description = \"Custom Plugin for S3 Sink\" content_type = \"ZIP\" s3_bucket_arn = aws_s3_bucket.archive.arn s3_file_key = aws_s3_object.s3_sink_msk_connect_custom_plugin_code.key timeouts = { create = \"60m\" } } } } resource \"aws_mskconnect_connector\" \"s3_sink\" { name = \"${var.msk-cluster-name}-archiver\" kafkaconnect_version = \"2.7.1\" capacity { provisioned_capacity { worker_count = 1 } } # Assumes JSON data and partitioned by the hour # See https://docs.confluent.io/kafka-connectors/s3-sink/current/configuration_options.html connector_configuration = { \"connector.class\" = \"io.confluent.connect.s3.S3SinkConnector\" \"s3.region\" = \"us-west-2\" , \"format.class\" = \"io.confluent.connect.s3.format.bytearray.ByteArrayFormat\" , \"value.converter\" = \"org.apache.kafka.connect.converters.ByteArrayConverter\" , \"format.bytearray.extension\" = \".json\" , \"path.format\" = \"'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH\" \"flush.size\" = \"1\" , \"schema.compatibility\" = \"NONE\" , \"topics\" = var.archived-topics , \"tasks.max\" = \"1\" , \"partitioner.class\" = \"io.confluent.connect.storage.partitioner.TimeBasedPartitioner\" , \"locale\" = \"en-GB\" , \"timezone\" = \"UTC\" , \"partition.duration.ms\" = \"3600000\" \"storage.class\" = \"io.confluent.connect.s3.storage.S3Storage\" , \"s3.bucket.name\" = aws_s3_bucket.archive.id , } kafka_cluster { apache_kafka_cluster { bootstrap_servers = module.msk_cluster.bootstrap_brokers_tls vpc { security_groups = [ module.security_group.security_group_id ] subnets = [ data.aws_subnets.subnets.ids.0 , data.aws_subnets.subnets.ids.1 ] } } } kafka_cluster_client_authentication { authentication_type = \"NONE\" } kafka_cluster_encryption_in_transit { encryption_type = \"TLS\" } plugin { custom_plugin { arn = module.msk_cluster.connect_custom_plugins.s3_sink.arn revision = module.msk_cluster.connect_custom_plugins.s3_sink.latest_revision } } log_delivery { worker_log_delivery { cloudwatch_logs { enabled = true log_group = aws_cloudwatch_log_group.kafka-archiver-logs.name } } } service_execution_role_arn = aws_iam_role.connector_role.arn } resource \"aws_cloudwatch_log_group\" \"kafka-archiver-logs\" { name = \"${var.msk-cluster-name}-archiver\" } resource \"aws_iam_role\" \"connector_role\" { name = \"${var.msk-cluster-name}-archiver-role\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"kafkaconnect.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] }) } resource \"aws_iam_role_policy\" \"connector_role_policy\" { name = \"${var.msk-cluster-name}-archiver-role-policy\" role = aws_iam_role.connector_role.id policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListAllMyBuckets\" ], \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" , \"s3:GetBucketLocation\" , \"s3:DeleteObject\" ], \"Resource\" : \"arn:aws:s3:::*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:AbortMultipartUpload\" , \"s3:ListMultipartUploadParts\" , \"s3:ListBucketMultipartUploads\" ], \"Resource\" : \"*\" } ] }) }","tags":"Cloud","url":"https://codesnips.pro/kafka-with-aws-msk-and-an-s3-archive-via-kafka-connect/","loc":"https://codesnips.pro/kafka-with-aws-msk-and-an-s3-archive-via-kafka-connect/"},{"title":"PHP","text":"Querying PostgreSQL from PHP","tags":"PHP","url":"https://codesnips.pro/php/","loc":"https://codesnips.pro/php/"},{"title":"Plotting scatter graph of CSV data via matplotlib","text":"import pandas as pd import matplotlib.pyplot as plt # Read data from CSV file into a pandas DataFrame data = pd . read_csv ( 'data.csv' ) # Extract x and y values from the DataFrame x_values = data [ 'x_values' ] y_values = data [ 'y_values' ] # Create the scatter plot plt . scatter ( x_values , y_values ) # Set labels and title plt . xlabel ( 'X-axis' ) plt . ylabel ( 'Y-axis' ) plt . title ( 'Scatter Plot' ) # Show the plot plt . show ()","tags":"Python","url":"https://codesnips.pro/plotting-scatter-graph-of-csv-data-via-matplotlib/","loc":"https://codesnips.pro/plotting-scatter-graph-of-csv-data-via-matplotlib/"},{"title":"Pushing a docker image to AWS Elastic Container Registry ( ECR )","text":"Makes use of Finding your AWS account ID via aws cli . # Configure to need IMAGE_NAME = myimage IMAGE_TAG = v1.0.0 AWS_REGION = us-west-1 # Get account ID to construct ECR tag AWS_ACCOUNT_ID = $( aws sts get-caller-identity --query \"Account\" --output text ) # Full image name and tag DOCKER_IMAGE = \" $AWS_ACCOUNT_ID .dkr.ecr. $AWS_REGION .amazonaws.com/ $IMAGE_NAME $ : $IMAGE_TAG \" # Assumes current dir has a Dockerfile docker build --tag $DOCKER_IMAGE . docker push $IMAGE","tags":"Cloud","url":"https://codesnips.pro/pushing-a-docker-image-to-aws-elastic-container-registry-ecr/","loc":"https://codesnips.pro/pushing-a-docker-image-to-aws-elastic-container-registry-ecr/"},{"title":"Python Flask Dockerfile","text":"# syntax=docker/dockerfile:1 FROM python:3.8-slim-buster WORKDIR /python-docker # Copy just requirements first to create cached layer with deps COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt # Now copy the rest of the source COPY . . CMD [ \"python3\" , \"-m\" , \"flask\" , \"run\" , \"--host=0.0.0.0\" ] Notes: Creating a .dockerignore is recommended to prevent the COPY . . line from copying too much (e.g. if you have a venv in the working directory)","tags":"Python","url":"https://codesnips.pro/python-flask-dockerfile/","loc":"https://codesnips.pro/python-flask-dockerfile/"},{"title":"Python","text":"Tools Using direnv on a Python project Writing Python tests for a cookiecutter template Flask Python Flask Dockerfile Hello World Flask App Reading a file line-by-line in Python Data Science Plotting scatter graph of CSV data via matplotlib Using GeoPandas to create points from latitude and longitude Using GeoPandas to group points into lines or paths","tags":"Python","url":"https://codesnips.pro/python/","loc":"https://codesnips.pro/python/"},{"title":"Quarkus Hello World Web app","text":"Prerequisites Create project skeleton using Maven: quarkus create app org.example:quarkus-hello-world \\ --extension = 'resteasy-reactive' See Creating Your First Application - Quarkus for more info. Code package org.example ; import javax.ws.rs.GET ; import javax.ws.rs.Path ; import javax.ws.rs.Produces ; import javax.ws.rs.core.MediaType ; @Path ( \"/hello\" ) public class HelloResource { @GET @Produces ( MediaType . TEXT_PLAIN ) public String hello () { return \"Hello, Quarkus!\" ; } }","tags":"Java","url":"https://codesnips.pro/quarkus-hello-world-web-app/","loc":"https://codesnips.pro/quarkus-hello-world-web-app/"},{"title":"Quarkus Multistage Dockerfile with Maven","text":"FROM quay.io/quarkus/centos-quarkus-maven:19.2.1 AS build COPY ./pom.xml ./pom.xml COPY ./src ./src RUN mvn -Pnative package -DskipTests FROM registry.access.redhat.com/ubi8/ubi-minimal WORKDIR /work/ COPY --from = build /project/target/*-runner /work/application RUN chmod 775 /work EXPOSE 8080 CMD [ \"./application\" , \"-Dquarkus.http.host=0.0.0.0\" ]","tags":"Java","url":"https://codesnips.pro/quarkus-multistage-dockerfile-with-maven/","loc":"https://codesnips.pro/quarkus-multistage-dockerfile-with-maven/"},{"title":"Quarkus Native Multistage Dockerfile with Maven","text":"## Stage 1 : build with maven builder image with native capabilities FROM quay.io/quarkus/ubi-quarkus-mandrel-builder-image:jdk-17 AS build COPY --chown = quarkus:quarkus mvnw /code/mvnw COPY --chown = quarkus:quarkus .mvn /code/.mvn COPY --chown = quarkus:quarkus pom.xml /code/ USER quarkus WORKDIR /code RUN ./mvnw -B org.apache.maven.plugins:maven-dependency-plugin:3.1.2:go-offline COPY src /code/src RUN ./mvnw package -Dnative ## Stage 2 : create the docker final image FROM quay.io/quarkus/quarkus-micro-image:2.0 WORKDIR /work/ COPY --from = build /code/target/*-runner /work/application # set up permissions for user `1001` RUN chmod 775 /work /work/application \\ && chown -R 1001 /work \\ && chmod -R \"g+rwX\" /work \\ && chown -R 1001 :root /work EXPOSE 8080 USER 1001 CMD [ \"./application\" , \"-Dquarkus.http.host=0.0.0.0\" ]","tags":"Java","url":"https://codesnips.pro/quarkus-native-multistage-dockerfile-with-maven/","loc":"https://codesnips.pro/quarkus-native-multistage-dockerfile-with-maven/"},{"title":"Querying PostgreSQL from PHP","text":"<?php // Database configuration $host = 'localhost' ; $dbname = 'your_database_name' ; $user = 'your_username' ; $password = 'your_password' ; try { // Create a new PDO instance $pdo = new PDO ( \"pgsql:host= $host ;dbname= $dbname \" , $user , $password ); // Set the PDO error mode to exception $pdo -> setAttribute ( PDO :: ATTR_ERRMODE , PDO :: ERRMODE_EXCEPTION ); // Prepare and execute a SELECT query $query = \"SELECT * FROM your_table_name\" ; $stmt = $pdo -> prepare ( $query ); $stmt -> execute (); // Fetch results as an associative array $results = $stmt -> fetchAll ( PDO :: FETCH_ASSOC ); // Loop through results and display data foreach ( $results as $row ) { echo \"ID: \" . $row [ 'id' ] . \", Name: \" . $row [ 'name' ] . \"<br>\" ; } } catch ( PDOException $e ) { echo \"Connection failed: \" . $e -> getMessage (); } ?>","tags":"PHP","url":"https://codesnips.pro/querying-postgresql-from-php/","loc":"https://codesnips.pro/querying-postgresql-from-php/"},{"title":"Reading a file line-by-line in Python","text":"file_path = 'example.txt' # Open the file in read mode ('r') with open ( file_path , 'r' ) as file : for line in file : print ( line . strip ())","tags":"Python","url":"https://codesnips.pro/reading-a-file-line-by-line-in-python/","loc":"https://codesnips.pro/reading-a-file-line-by-line-in-python/"},{"title":"Setting the font in xterm","text":"Add to $HOME/.Xresources : xterm*faceName: DejaVu Sans Mono Book xterm*faceSize: 11","tags":"Configs","url":"https://codesnips.pro/setting-the-font-in-xterm/","loc":"https://codesnips.pro/setting-the-font-in-xterm/"},{"title":"Using GeoPandas to create points from latitude and longitude","text":"import pandas as pd import geopandas as gpd # Read CSV with columns \"lon\" and \"lat\" as floats df = pd . read_csv ( 'my-csev.csv' ) df = gpd . GeoDataFrame ( df , geometry = gpd . points_from_xy ( df . lon , df . lat ), crs = \"EPSG:4326\" ) # Optionally (saves memory) drop original lat/lon columns df . drop ( 'lon' , axis = 1 , inplace = True ) df . drop ( 'lat' , axis = 1 , inplace = True ) See also Using GeoPandas to group points into lines or paths","tags":"Python","url":"https://codesnips.pro/using-geopandas-to-create-points-from-latitude-and-longitude/","loc":"https://codesnips.pro/using-geopandas-to-create-points-from-latitude-and-longitude/"},{"title":"Using GeoPandas to group points into lines or paths","text":"Assumes a pandas DataFrame with a column called \"geometry\". See Using GeoPandas to create points from latitude and longitude to create from latitude/longitude pairs. Useful if you have a series of points many-to-many mapped to some id ( some_id below) and the dataframe is ordered by that ID then by the order lines appear in the path. from shapely.geometry import LineString df = df . groupby ( 'some_id' )[ 'geometry' ] . apply ( lambda x : LineString ( x . tolist ()) ) . reset_index ( name = 'path' )","tags":"Python","url":"https://codesnips.pro/using-geopandas-to-group-points-into-lines-or-paths/","loc":"https://codesnips.pro/using-geopandas-to-group-points-into-lines-or-paths/"},{"title":"Using direnv on a Python project","text":"Prerequisites direnv is installed Snippet Create .envrc in project root: # Using direnv built-in to create venv layout python3 # Create venv as alias to latest # Useful for configuring ./venv as a fix dir in Pycharm etc. ln -sfn .direnv/ $( basename $VIRTUAL_ENV ) / venv # Optionally look for a 2nd file containing passwords # e.g. export API_KEY=xxx if file .envrc.secret ; then source_env .envrc.secret fi","tags":"Python","url":"https://codesnips.pro/using-direnv-on-a-python-project/","loc":"https://codesnips.pro/using-direnv-on-a-python-project/"},{"title":"Writing Python tests for a cookiecutter template","text":"Allow CI tests for a cookiecutter repo with some basic tests. Uses pytest-cookies so install that first: pip install pytest-cookies Tests below are for a Java project template (using Gradle). Adjust as desired. import os import shlex import subprocess from contextlib import contextmanager from cookiecutter.utils import rmtree @contextmanager def run_cookiecutter ( cookies , * args , ** kwargs ): \"\"\" Runs cookiecutter on entering `with` block. Deletes files when leaving `with` context \"\"\" result = cookies . bake ( * args , ** kwargs ) try : yield result finally : if result . project : rmtree ( str ( result . project )) @contextmanager def inside_dir ( dirpath ): \"\"\" Changes to given directory temporarily. \"\"\" old_path = os . getcwd () try : os . chdir ( dirpath ) yield finally : os . chdir ( old_path ) def run_inside_dir ( command , dirpath ): with inside_dir ( dirpath ): return subprocess . check_call ( shlex . split ( command )) def test_bake_with_defaults ( cookies ): with run_cookiecutter ( cookies ) as result : assert result . exception is None assert result . project . isdir () assert result . exit_code == 0 found_toplevel_files = [ f . basename for f in result . project . listdir () ] assert \"build.gradle.kts\" in found_toplevel_files def test_bake_and_run_build ( cookies ): with run_cookiecutter ( cookies ) as result : assert result . exception is None assert result . project . isdir () assert run_inside_dir ( \"gradle build\" , str ( result . project )) == 0 def test_setting_project_name ( cookies ): properties = { \"repo_name\" : \"foo-service\" } with run_cookiecutter ( cookies , extra_context = properties ) as result : assert result . exception is None assert result . exit_code == 0 assert \"foo-service\" == result . project . basename with open ( str ( result . project . join ( \"settings.gradle.kts\" ))) as manifest_file : assert \"foo-service\" in manifest_file . read ()","tags":"Python","url":"https://codesnips.pro/writing-python-tests-for-a-cookiecutter-template/","loc":"https://codesnips.pro/writing-python-tests-for-a-cookiecutter-template/"},{"title":"Welcome to CodeSnips","text":"Working code snippets for the busy professional. Use the search or browse by category: Cloud Java Python Configs PHP","tags":"misc","url":"https://codesnips.pro///","loc":"https://codesnips.pro///"}]};